{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8624dfca",
   "metadata": {},
   "source": [
    "Create a dataset from 3 numpy files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b7d0f",
   "metadata": {},
   "source": [
    "Import numpy file for each shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e5c10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(616, 1083, 1035)\n",
      "(535, 1083, 1035)\n",
      "(691, 1083, 1035)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# circle = np.load(\"../data/circle/circle_data_preprocess.npy\")    # (N1, 128, 128)  # (N1, 128, 128)\n",
    "# square = np.load(\"../data/square/square_data_preprocess.npy\")    # (N2, 128, 128)\n",
    "# triangle = np.load(\"../data/mini_dataset/triangle/triangle_data_preprocess.npy\")# (N3, 128, 128)\n",
    "base_dir = \"D:/data_saxs_LPS/reduced_dataset\"\n",
    "\n",
    "path_circle = os.path.join(base_dir,\"circle/first_channel.npy\")\n",
    "path_square = os.path.join(base_dir,\"square/first_channel.npy\")\n",
    "path_triangle = os.path.join(base_dir,\"triangle/first_channel.npy\")\n",
    "\n",
    "\n",
    "\n",
    "# path_circle = \"D:/data_saxs_LPS/reduced_dataset/circle/circle_data_preprocess.npy\"\n",
    "# path_square = \"D:/data_saxs_LPS/reduced_dataset/square/square_data_preprocess.npy\"\n",
    "# path_triangle = \"D:/data_saxs_LPS/reduced_dataset/triangle/triangle_data_preprocess.npy\"\n",
    "\n",
    "circle = np.load(path_circle)\n",
    "square = np.load(path_square)    # (N2, 128, 128)\n",
    "triangle = np.load(path_triangle)# (N3, 128, 128\n",
    "\n",
    "# circle = np.load(\"D:/data_saxs_LPS/reduced_dataset/circle/first_channel.npy\")\n",
    "# square = np.load(\"D:/data_saxs_LPS/reduced_dataset/square/first_channel.npy\")    # (N2, 128, 128)\n",
    "# triangle = np.load(\"D:/data_saxs_LPS/reduced_dataset/triangle/first_channel.npy\")# (N3, 128, 128\n",
    "\n",
    "\n",
    "\n",
    "print(circle.shape)\n",
    "print(square.shape)\n",
    "print(triangle.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabc788",
   "metadata": {},
   "source": [
    "Create corresponding labels.\n",
    "Create 3 numpy array:\n",
    "- 1 filled with 0 for circles, the shape is `len(circle)`\n",
    "- 1 filled with 1 for squares, the shape is `len(square)`\n",
    "- 1 filled with 2 for triangles, the shape is `len(triangle)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c8eeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(616,)\n",
      "(535,)\n",
      "(691,)\n"
     ]
    }
   ],
   "source": [
    "y_circle = np.zeros(len(circle), dtype=int)           # classe 0\n",
    "y_square = np.ones(len(square), dtype=int)            # classe 1\n",
    "y_triangle = np.full(len(triangle), 2, dtype=int) \n",
    "\n",
    "print(y_circle.shape)\n",
    "print(y_square.shape)\n",
    "print(y_triangle.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d685a",
   "metadata": {},
   "source": [
    "Concatenante the 3 arrays to pass from shapes (x,) (y,) (z,) to (x+y+z,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952179f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1842,)\n"
     ]
    }
   ],
   "source": [
    "y = np.concatenate([y_circle, y_square, y_triangle], axis=0)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adbe023",
   "metadata": {},
   "source": [
    "Concatenate the 3 numpy array to pass from dimensions (x, 128, 128) (y, 128, 128) (y, 128, 128) to 1 array of shape (x+y+z,128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7508c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1842, 1083, 1035)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate([circle, square, triangle], axis=0) \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ffbd1",
   "metadata": {},
   "source": [
    "Keras need data in the shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ff9882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1842, 1083, 1035)\n",
      "(1842, 1083, 1035, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X = X[..., np.newaxis]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7291576",
   "metadata": {},
   "source": [
    "OPTIONAL: Set random seed to (for reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b0339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55594c7",
   "metadata": {},
   "source": [
    "Shuffle with scikit learn to avoid learning on blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695f28ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 ... 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, y = shuffle(X, y, random_state=seed)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57706c0",
   "metadata": {},
   "source": [
    "Split dataset in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d980908d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1473, 1083, 1035, 1)\n",
      "(369, 1083, 1035, 1)\n",
      "(1473,)\n",
      "(369,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose que X et y sont déjà construits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,         # 20% en test\n",
    "    random_state=seed,       # pour reproductibilité\n",
    "    stratify=y             # pour garder la proportion des classes\n",
    ")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247189ed",
   "metadata": {},
   "source": [
    "Create a CNN 2D Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c7d550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cayez\\Documents\\DONNEES_SIMON\\python_env\\donnees\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# --- ton modèle ---\n",
    "model = models.Sequential([\n",
    "   # layers.Conv2D(32, (3,3), activation='relu', input_shape=(128,128,1)),\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(1083,1035,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')  # 3 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4891fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "AbortedError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall/gradient_tape/sequential_1/max_pooling2d_1/MaxPool2d/MaxPoolGrad defined at (most recent call last):\n<stack traces unavailable>\nCompute received an exception:Status:1, message: could not create a memory object. in file tensorflow/core/kernels/mkl/mkl_maxpooling_op.cc:388\n\t [[{{node StatefulPartitionedCall/gradient_tape/sequential_1/max_pooling2d_1/MaxPool2d/MaxPoolGrad}}]] [Op:__inference_multi_step_on_iterator_1637]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAbortedError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- entraînement ---\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# tes données d'entraînement\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# nombre d'epochs\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# taille de batch\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# fraction du train utilisée pour validation\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# affiche les métriques\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# --- tracé des courbes de métriques ---\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cayez\\Documents\\DONNEES_SIMON\\python_env\\donnees\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\cayez\\Documents\\DONNEES_SIMON\\python_env\\donnees\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAbortedError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall/gradient_tape/sequential_1/max_pooling2d_1/MaxPool2d/MaxPoolGrad defined at (most recent call last):\n<stack traces unavailable>\nCompute received an exception:Status:1, message: could not create a memory object. in file tensorflow/core/kernels/mkl/mkl_maxpooling_op.cc:388\n\t [[{{node StatefulPartitionedCall/gradient_tape/sequential_1/max_pooling2d_1/MaxPool2d/MaxPoolGrad}}]] [Op:__inference_multi_step_on_iterator_1637]"
     ]
    }
   ],
   "source": [
    "# --- entraînement ---\n",
    "history = model.fit(\n",
    "    X_train, y_train,          # tes données d'entraînement\n",
    "    epochs=20,                 # nombre d'epochs\n",
    "    batch_size=64,             # taille de batch\n",
    "    validation_split=0.2,      # fraction du train utilisée pour validation\n",
    "    verbose=1                  # affiche les métriques\n",
    ")\n",
    "\n",
    "# --- tracé des courbes de métriques ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927fc7a",
   "metadata": {},
   "source": [
    "Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- prédiction sur le jeu de test ---\n",
    "y_pred_proba = model.predict(X_test)              # probabilités (nb_images, 3)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)          # classes prédites (0,1,2)\n",
    "\n",
    "# --- accuracy ---\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy sur le jeu de test : {acc:.3f}\")\n",
    "\n",
    "# --- matrice de confusion ---\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matrice de confusion :\")\n",
    "print(cm)\n",
    "\n",
    "# --- affichage joli ---\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['circle','square','triangle'],\n",
    "            yticklabels=['circle','square','triangle'])\n",
    "plt.xlabel('Prédit')\n",
    "plt.ylabel('Réel')\n",
    "plt.title(f'Matrice de confusion (Accuracy {acc:.2%})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ecba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model : ton modèle Keras\n",
    "def model_size_and_params(model, dtype_size=4):\n",
    "    params = model.count_params()\n",
    "    # approx size in bytes (float32 = 4 bytes)\n",
    "    size_bytes = params * dtype_size\n",
    "    size_mb = size_bytes / (1024**2)\n",
    "    return params, size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65825dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def file_size_mb(path):\n",
    "    \"\"\"Retourne la taille d’un fichier en MB\"\"\"\n",
    "    return os.path.getsize(path) / (1024 ** 2)\n",
    "\n",
    "circle_size   = file_size_mb(path_circle)\n",
    "square_size   = file_size_mb(path_square)\n",
    "triangle_size = file_size_mb(path_triangle)\n",
    "\n",
    "\n",
    "def measure_inference_time(model, input_shape, n_warmup=10, n_run=100):\n",
    "    # generate random input (batch size 1)\n",
    "    x = np.random.rand(1, *input_shape).astype(np.float32)\n",
    "    # warmup\n",
    "    for _ in range(n_warmup):\n",
    "        _ = model.predict(x, verbose=0)\n",
    "    # timed runs\n",
    "    t0 = time.time()\n",
    "    for _ in range(n_run):\n",
    "        _ = model.predict(x, verbose=0)\n",
    "    t1 = time.time()\n",
    "    avg_ms = (t1 - t0) / n_run * 1000\n",
    "    return avg_ms\n",
    "\n",
    "print(f\"Taille circle:   {circle_size:.2f} MB\")\n",
    "print(f\"Taille square:   {square_size:.2f} MB\")\n",
    "print(f\"Taille triangle: {triangle_size:.2f} MB\")\n",
    "print(f\"Taille totale:   {circle_size + square_size + triangle_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume model, X_train, folder_path exist\n",
    "params, model_mb = model_size_and_params(model)\n",
    "print(\"Params:\", params)\n",
    "print(\"Model size (MB):\", model_mb)\n",
    "print(f\"Taille totale:   {circle_size + square_size + triangle_size:.2f} MB\")\n",
    "avg_infer_ms = measure_inference_time(model, X_train.shape[1:])\n",
    "print(\"Inference ms:\", avg_infer_ms)\n",
    "# training time demo (1 epoch)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(X_train[:128], y_train[:128], epochs=1, batch_size=8, verbose=1)\n",
    "print(\"1 epoch time (s):\", time.time()-t0)\n",
    "import psutil, os\n",
    "print(\"Process RSS (MB):\", psutil.Process(os.getpid()).memory_info().rss / 1024**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fe4eb7",
   "metadata": {},
   "source": [
    "For pre-processed data:   \n",
    "<pre>\n",
    "Params: 3705475\n",
    "Model size (MB): 14.135265350341797\n",
    "Taille totale:   230.25 MB\n",
    "Inference ms: 710.1784157752991\n",
    "16/16 ━━━━━━━━━━━━━━━━━━━━ 3s 148ms/step - accuracy: 0.8729 - loss: 0.3561\n",
    "1 epoch time (s): 3.1816084384918213\n",
    "Process RSS (MB): 1528.75390625\n",
    "</pre>\n",
    "---\n",
    "<pre>\n",
    "Params: 3705475\n",
    "Model size (MB): 14.135265350341797\n",
    "Taille totale:   230.25 MB\n",
    "Inference ms: 561.4975476264954\n",
    "16/16 ━━━━━━━━━━━━━━━━━━━━ 8s 182ms/step - accuracy: 0.3554 - loss: 1.4452\n",
    "1 epoch time (s): 8.540453910827637\n",
    "Process RSS (MB): 1283.8359375\n",
    "</pre>\n",
    "---\n",
    "---\n",
    "For raw data\n",
    "\n",
    "<pre>\n",
    "Params: 283187843\n",
    "Model size (MB): 1080.2758903503418\n",
    "Taille totale:   7876.23 MB\n",
    "Inference ms: 940.4475092887878\n",
    "16/16 ━━━━━━━━━━━━━━━━━━━━ 219s 11s/step - accuracy: 0.4024 - loss: 18972995584.0000\n",
    "1 epoch time (s): 232.40875720977783\n",
    "Process RSS (MB): 11297.3125\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "donnees",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
